{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacetimeformer as stf\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from scipy.stats import chi2, norm, beta, gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    # fixed but maybe change...\n",
    "    'batch_size':2000,\n",
    "    'workers':1,\n",
    "    'init_lr':1e-10,\n",
    "    'base_lr':0.0005,\n",
    "    'context_points':32,\n",
    "    'target_points':1,\n",
    "    'd_model':100,\n",
    "    'd_qk':100,\n",
    "    'd_v':100,\n",
    "    'd_ff':400,\n",
    "    'n_heads':1,\n",
    "    'enc_layers':1,\n",
    "    'dec_layers':1,\n",
    "    'global_self_attn':'full',\n",
    "    'local_self_attn':'full',\n",
    "    'global_cross_attn':'full',\n",
    "    'local_cross_attn':'full',\n",
    "    'no_val':False,\n",
    "    'no_time':False,\n",
    "    'no_space':False,\n",
    "    'no_given':False,\n",
    "\n",
    "    # directly set parameters\n",
    "    'gpus':[0],\n",
    "    # 'gpus':None,\n",
    "    'strategy':'dp',\n",
    "    \"time_resolution\": 1,\n",
    "    \"start_token_len\": 0,\n",
    "    \"attn_factor\": 5,\n",
    "    \"dropout_emb\": 0.2,\n",
    "    \"dropout_attn_out\": 0,\n",
    "    \"dropout_attn_matrix\": 0,\n",
    "    \"dropout_qkv\": 0,\n",
    "    \"dropout_ff\": 0.3,\n",
    "    \"pos_emb_type\": 'abs',\n",
    "    \"no_final_norm\": False,\n",
    "    \"performer_kernel\": 'relu',\n",
    "    \"performer_redraw_interval\": 100,\n",
    "    \"attn_time_windows\": 1,\n",
    "    \"use_shifted_time_windows\": False,\n",
    "    \"norm\": 'batch',\n",
    "    \"activation\": 'gelu',\n",
    "    \"warmup_steps\": 0,\n",
    "    \"decay_factor\": 0.25,\n",
    "    \"initial_downsample_convs\": 0,\n",
    "    \"intermediate_downsample_convs\": 0,\n",
    "    \"embed_method\": 'spatio-temporal',\n",
    "    \"l2_coeff\": 0.000001,\n",
    "    \"loss\": 'mse',\n",
    "    \"class_loss_imp\": 0.1,\n",
    "    \"recon_loss_imp\": 0,\n",
    "    \"time_emb_dim\": 3,\n",
    "    \"null_value\": None,\n",
    "    \"pad_value\": None,\n",
    "    \"linear_window\": 0,\n",
    "    \"use_revin\": False,\n",
    "    \"linear_shared_weights\": False,\n",
    "    \"use_seasonal_decomp\": False,\n",
    "    \"recon_mask_skip_all\": 1,\n",
    "    \"recon_mask_max_seq_len\": 5,\n",
    "    \"recon_mask_drop_seq\": 0.2,\n",
    "    \"recon_mask_drop_standard\": 0.1,\n",
    "    \"recon_mask_drop_full\": 0.05,\n",
    "}\n",
    "\n",
    "def create_model(config, x_dim, yc_dim, yt_dim):\n",
    "    max_seq_len = config['context_points'] + config['target_points']\n",
    "\n",
    "    forecaster = stf.spacetimeformer_model.Spacetimeformer_Forecaster(\n",
    "        d_x=x_dim,\n",
    "        d_yc=yc_dim,\n",
    "        d_yt=yt_dim,\n",
    "        max_seq_len=max_seq_len,    \n",
    "        start_token_len=config['start_token_len'],\n",
    "        attn_factor=config['attn_factor'],\n",
    "        d_model=config['d_model'],\n",
    "        d_queries_keys=config['d_qk'],\n",
    "        d_values=config['d_v'],\n",
    "        n_heads=config['n_heads'],\n",
    "        e_layers=config['enc_layers'],\n",
    "        d_layers=config['dec_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        dropout_emb=config['dropout_emb'],\n",
    "        dropout_attn_out=config['dropout_attn_out'],\n",
    "        dropout_attn_matrix=config['dropout_attn_matrix'],\n",
    "        dropout_qkv=config['dropout_qkv'],\n",
    "        dropout_ff=config['dropout_ff'],\n",
    "        pos_emb_type=config['pos_emb_type'],\n",
    "        use_final_norm=not config['no_final_norm'],\n",
    "        global_self_attn=config['global_self_attn'],\n",
    "        local_self_attn=config['local_self_attn'],\n",
    "        global_cross_attn=config['global_cross_attn'],\n",
    "        local_cross_attn=config['local_cross_attn'],\n",
    "        performer_kernel=config['performer_kernel'],\n",
    "        performer_redraw_interval=config['performer_redraw_interval'],\n",
    "        attn_time_windows=config['attn_time_windows'],\n",
    "        use_shifted_time_windows=config['use_shifted_time_windows'],\n",
    "        norm=config['norm'],\n",
    "        activation=config['activation'],\n",
    "        init_lr=config['init_lr'],\n",
    "        base_lr=config['base_lr'],\n",
    "        warmup_steps=config['warmup_steps'],\n",
    "        decay_factor=config['decay_factor'],\n",
    "        initial_downsample_convs=config['initial_downsample_convs'],\n",
    "        intermediate_downsample_convs=config['intermediate_downsample_convs'],\n",
    "        embed_method=config['embed_method'],\n",
    "        l2_coeff=config['l2_coeff'],\n",
    "        loss=config['loss'],\n",
    "        class_loss_imp=config['class_loss_imp'],\n",
    "        recon_loss_imp=config['recon_loss_imp'],\n",
    "        time_emb_dim=config['time_emb_dim'],\n",
    "        null_value=config['null_value'],\n",
    "        pad_value=config['pad_value'],\n",
    "        linear_window=config['linear_window'],\n",
    "        use_revin=config['use_revin'],\n",
    "        linear_shared_weights=config['linear_shared_weights'],\n",
    "        use_seasonal_decomp=config['use_seasonal_decomp'],\n",
    "        use_val=not config['no_val'],\n",
    "        use_time=not config['no_time'],\n",
    "        use_space=not config['no_space'],\n",
    "        use_given=not config['no_given'],\n",
    "        recon_mask_skip_all=config['recon_mask_skip_all'],\n",
    "        recon_mask_max_seq_len=config['recon_mask_max_seq_len'],\n",
    "        recon_mask_drop_seq=config['recon_mask_drop_seq'],\n",
    "        recon_mask_drop_standard=config['recon_mask_drop_standard'],\n",
    "        recon_mask_drop_full=config['recon_mask_drop_full'],\n",
    "    )\n",
    "    return forecaster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_data(config):\n",
    "    fs = 2048  # Sampling rate (Hz)\n",
    "    T = 150  # Length of epochs (s)\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Define the number of iterations for the simulation\n",
    "    n_iterations = fs * T\n",
    "\n",
    "    # Preallocate the arrays for the x variables\n",
    "    x1 = np.zeros(n_iterations)\n",
    "    x2 = np.zeros(n_iterations)\n",
    "    x3 = np.zeros(n_iterations)\n",
    "    x4 = np.zeros(n_iterations)\n",
    "    x5 = np.zeros(n_iterations)\n",
    "\n",
    "    # Define the rate lambda for the exponential distribution\n",
    "    lambda_rate = 2\n",
    "\n",
    "    # Generate the noise processes e1t, e2t, e3t, e4t, e5t\n",
    "    e1 = norm.rvs(scale=1, size=n_iterations)  # Gaussian with mean 0, std 1\n",
    "    e2 = np.random.exponential(scale=1 / lambda_rate, size=n_iterations)\n",
    "    e3 = beta.rvs(a=1, b=2, size=n_iterations)\n",
    "    e4 = beta.rvs(a=2, b=1, size=n_iterations)\n",
    "    e5 = norm.rvs(scale=1, size=n_iterations)  # Gaussian with mean 0, std 1\n",
    "\n",
    "    for t in range(3, n_iterations):\n",
    "        # Generate the x variables based on the given equations\n",
    "        x1[t] = 0.7 * x1[t - 1] + e1[t]\n",
    "        x2[t] = 0.3 * np.power(x1[t - 2], 2) + e2[t]\n",
    "        x3[t] = 0.4 * x1[t-3] - 0.3 * x3[t-2] + e3[t]\n",
    "        x4[t] = 0.7 * x4[t-1] - 0.3 * x5[t-1] * np.exp((-math.pow(x5[t-1], 2)) / 2) + e4[t]\n",
    "        x5[t] = 0.5 * x4[t-1] + 0.2 * x5[t-2] + e5[t]\n",
    "\n",
    "    data = np.array([x1, x2, x3, x4, x5]).T\n",
    "    PLOT_VAR_NAMES = np.arange(5) + 1\n",
    "    PLOT_VAR_IDXS = np.arange(5)\n",
    "    df = pd.DataFrame(data, columns=PLOT_VAR_NAMES)\n",
    "    df[\"Datetime\"] = pd.date_range(start=\"1/1/2020\", periods=df.shape[0], freq=\"ms\")\n",
    "\n",
    "    dset = stf.data.CSVTimeSeries(\n",
    "        data_path=None,\n",
    "        raw_df=df,\n",
    "        val_split=0.2,\n",
    "        test_split=0.2,\n",
    "        normalize=True,\n",
    "        time_col_name=\"Datetime\",\n",
    "        time_features=[\"minute\", 'second', 'millisecond'],\n",
    "    )\n",
    "    yc_dim = data.shape[1]\n",
    "    yt_dim = data.shape[1]\n",
    "    x_dim = dset.time_cols.shape[0]\n",
    "\n",
    "    DATA_MODULE = stf.data.DataModule(\n",
    "        datasetCls=stf.data.CSVTorchDset,\n",
    "        dataset_kwargs={\n",
    "            \"csv_time_series\": dset,\n",
    "            \"context_points\": config.context_points,\n",
    "            \"target_points\": config.target_points,\n",
    "            \"time_resolution\": config.time_resolution,\n",
    "        },\n",
    "        batch_size=config.batch_size,\n",
    "        workers=config.workers,\n",
    "        overfit=False,\n",
    "    )\n",
    "\n",
    "    return DATA_MODULE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run-20240131_113012-94308ezc == s2\n",
    "\n",
    "run-20240131_120438-yj71byzb == s2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecaster\n",
      "\tL2: 1e-06\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n",
      "GlobalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "GlobalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "LocalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "LocalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "Using Embedding: spatio-temporal\n",
      "Time Emb Dim: 3\n",
      "Space Embedding: True\n",
      "Time Embedding: True\n",
      "Val Embedding: True\n",
      "Given Embedding: True\n",
      "Null Value: None\n",
      "Pad Value: None\n",
      "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1\n",
      " *** Spacetimeformer (v1.5) Summary: *** \n",
      "\t\tModel Dim: 100\n",
      "\t\tFF Dim: 400\n",
      "\t\tEnc Layers: 1\n",
      "\t\tDec Layers: 1\n",
      "\t\tEmbed Dropout: 0.2\n",
      "\t\tFF Dropout: 0.3\n",
      "\t\tAttn Out Dropout: 0\n",
      "\t\tAttn Matrix Dropout: 0\n",
      "\t\tQKV Dropout: 0\n",
      "\t\tL2 Coeff: 1e-06\n",
      "\t\tWarmup Steps: 0\n",
      "\t\tNormalization Scheme: batch\n",
      "\t\tAttention Time Windows: 1\n",
      "\t\tShifted Time Windows: False\n",
      "\t\tPosition Emb Type: abs\n",
      "\t\tRecon Loss Imp: 0\n",
      " ***                                  *** \n",
      "Forecaster\n",
      "\tL2: 1e-06\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n",
      "GlobalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "GlobalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Using Embedding: spatio-temporal\n",
      "Time Emb Dim: 6\n",
      "Space Embedding: True\n",
      "Time Embedding: True\n",
      "Val Embedding: True\n",
      "Given Embedding: True\n",
      "Null Value: None\n",
      "Pad Value: None\n",
      "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
      " *** Spacetimeformer (v1.5) Summary: *** \n",
      "\t\tModel Dim: 100\n",
      "\t\tFF Dim: 400\n",
      "\t\tEnc Layers: 1\n",
      "\t\tDec Layers: 1\n",
      "\t\tEmbed Dropout: 0.2\n",
      "\t\tFF Dropout: 0.3\n",
      "\t\tAttn Out Dropout: 0.0\n",
      "\t\tAttn Matrix Dropout: 0.0\n",
      "\t\tQKV Dropout: 0.0\n",
      "\t\tL2 Coeff: 1e-06\n",
      "\t\tWarmup Steps: 0\n",
      "\t\tNormalization Scheme: batch\n",
      "\t\tAttention Time Windows: 1\n",
      "\t\tShifted Time Windows: False\n",
      "\t\tPosition Emb Type: abs\n",
      "\t\tRecon Loss Imp: 0.0\n",
      " ***                                  *** \n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "\n",
    "model = create_model(default_config, 3, 5, 5)\n",
    "s2 = \"/home/dan/Documents/images/singleTokenPreds/paper_nets/S2_08917f58/S2epoch=09.ckpt\"\n",
    "s2 = model.load_from_checkpoint(s2)\n",
    "# results = trainer.test(model=model2, datamodule=my_datamodule, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
