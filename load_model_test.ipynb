{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacetimeformer as stf\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from scipy.stats import chi2, norm, beta, gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/miniconda3/envs/stf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='dp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='dp')` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "default_config = {\n",
    "    # fixed but maybe change...\n",
    "    'batch_size': 100,\n",
    "    'workers': 6,\n",
    "    'init_lr': 1e-10,\n",
    "    'base_lr': 0.0005,\n",
    "    'context_points': 32,\n",
    "    'target_points': 1,\n",
    "    'd_model': 100,\n",
    "    'd_qk': 100,\n",
    "    'd_v': 100,\n",
    "    'd_ff': 400,\n",
    "    'n_heads': 1,\n",
    "    'enc_layers': 1,\n",
    "    'dec_layers': 1,\n",
    "    'global_self_attn': 'full',\n",
    "    'local_self_attn': 'full',\n",
    "    'global_cross_attn': 'full',\n",
    "    'local_cross_attn': 'full',\n",
    "    'no_val': False,\n",
    "    'no_time': False,\n",
    "    'no_space': False,\n",
    "    'no_given': False,\n",
    "\n",
    "    # directly set parameters\n",
    "    'gpus': [0],\n",
    "    # 'gpus':None,\n",
    "    'strategy': 'dp',\n",
    "    \"time_resolution\": 1,\n",
    "    \"start_token_len\": 0,\n",
    "    \"attn_factor\": 5,\n",
    "    \"dropout_emb\": 0.2,\n",
    "    \"dropout_attn_out\": 0,\n",
    "    \"dropout_attn_matrix\": 0,\n",
    "    \"dropout_qkv\": 0,\n",
    "    \"dropout_ff\": 0.3,\n",
    "    \"pos_emb_type\": 'abs',\n",
    "    \"no_final_norm\": False,\n",
    "    \"performer_kernel\": 'relu',\n",
    "    \"performer_redraw_interval\": 100,\n",
    "    \"attn_time_windows\": 1,\n",
    "    \"use_shifted_time_windows\": False,\n",
    "    \"norm\": 'batch',\n",
    "    \"activation\": 'gelu',\n",
    "    \"warmup_steps\": 0,\n",
    "    \"decay_factor\": 0.25,\n",
    "    \"initial_downsample_convs\": 0,\n",
    "    \"intermediate_downsample_convs\": 0,\n",
    "    \"embed_method\": 'spatio-temporal',\n",
    "    \"l2_coeff\": 0.000001,\n",
    "    \"loss\": 'mse',\n",
    "    \"class_loss_imp\": 0.1,\n",
    "    \"recon_loss_imp\": 0,\n",
    "    \"time_emb_dim\": 3,\n",
    "    \"null_value\": None,\n",
    "    \"pad_value\": None,\n",
    "    \"linear_window\": 0,\n",
    "    \"use_revin\": False,\n",
    "    \"linear_shared_weights\": False,\n",
    "    \"use_seasonal_decomp\": False,\n",
    "    \"recon_mask_skip_all\": 1,\n",
    "    \"recon_mask_max_seq_len\": 5,\n",
    "    \"recon_mask_drop_seq\": 0.2,\n",
    "    \"recon_mask_drop_standard\": 0.1,\n",
    "    \"recon_mask_drop_full\": 0.05,\n",
    "    \"grad_clip_norm\": 0.0,\n",
    "    \"accumulate\": 1,\n",
    "    \"limit_val_batches\": 1.0,\n",
    "    \"max_epochs\": 10,\n",
    "    \"val_check_interval\": 1.0,\n",
    "}\n",
    "\n",
    "def create_model(config, x_dim, yc_dim, yt_dim):\n",
    "    max_seq_len = config['context_points'] + config['target_points']\n",
    "\n",
    "    forecaster = stf.spacetimeformer_model.Spacetimeformer_Forecaster(\n",
    "        d_x=x_dim,\n",
    "        d_yc=yc_dim,\n",
    "        d_yt=yt_dim,\n",
    "        max_seq_len=max_seq_len,\n",
    "        start_token_len=config['start_token_len'],\n",
    "        attn_factor=config['attn_factor'],\n",
    "        d_model=config['d_model'],\n",
    "        d_queries_keys=config['d_qk'],\n",
    "        d_values=config['d_v'],\n",
    "        n_heads=config['n_heads'],\n",
    "        e_layers=config['enc_layers'],\n",
    "        d_layers=config['dec_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        dropout_emb=config['dropout_emb'],\n",
    "        dropout_attn_out=config['dropout_attn_out'],\n",
    "        dropout_attn_matrix=config['dropout_attn_matrix'],\n",
    "        dropout_qkv=config['dropout_qkv'],\n",
    "        dropout_ff=config['dropout_ff'],\n",
    "        pos_emb_type=config['pos_emb_type'],\n",
    "        use_final_norm=not config['no_final_norm'],\n",
    "        global_self_attn=config['global_self_attn'],\n",
    "        local_self_attn=config['local_self_attn'],\n",
    "        global_cross_attn=config['global_cross_attn'],\n",
    "        local_cross_attn=config['local_cross_attn'],\n",
    "        performer_kernel=config['performer_kernel'],\n",
    "        performer_redraw_interval=config['performer_redraw_interval'],\n",
    "        attn_time_windows=config['attn_time_windows'],\n",
    "        use_shifted_time_windows=config['use_shifted_time_windows'],\n",
    "        norm=config['norm'],\n",
    "        activation=config['activation'],\n",
    "        init_lr=config['init_lr'],\n",
    "        base_lr=config['base_lr'],\n",
    "        warmup_steps=config['warmup_steps'],\n",
    "        decay_factor=config['decay_factor'],\n",
    "        initial_downsample_convs=config['initial_downsample_convs'],\n",
    "        intermediate_downsample_convs=config['intermediate_downsample_convs'],\n",
    "        embed_method=config['embed_method'],\n",
    "        l2_coeff=config['l2_coeff'],\n",
    "        loss=config['loss'],\n",
    "        class_loss_imp=config['class_loss_imp'],\n",
    "        recon_loss_imp=config['recon_loss_imp'],\n",
    "        time_emb_dim=config['time_emb_dim'],\n",
    "        null_value=config['null_value'],\n",
    "        pad_value=config['pad_value'],\n",
    "        linear_window=config['linear_window'],\n",
    "        use_revin=config['use_revin'],\n",
    "        linear_shared_weights=config['linear_shared_weights'],\n",
    "        use_seasonal_decomp=config['use_seasonal_decomp'],\n",
    "        use_val=not config['no_val'],\n",
    "        use_time=not config['no_time'],\n",
    "        use_space=not config['no_space'],\n",
    "        use_given=not config['no_given'],\n",
    "        recon_mask_skip_all=config['recon_mask_skip_all'],\n",
    "        recon_mask_max_seq_len=config['recon_mask_max_seq_len'],\n",
    "        recon_mask_drop_seq=config['recon_mask_drop_seq'],\n",
    "        recon_mask_drop_standard=config['recon_mask_drop_standard'],\n",
    "        recon_mask_drop_full=config['recon_mask_drop_full'],\n",
    "    )\n",
    "    return forecaster\n",
    "\n",
    "def s2_data(config):\n",
    "    fs = 2048  # Sampling rate (Hz)\n",
    "    T = 150  # Length of epochs (s)\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Define the number of iterations for the simulation\n",
    "    n_iterations = fs * T\n",
    "\n",
    "    # Preallocate the arrays for the x variables\n",
    "    x1 = np.zeros(n_iterations)\n",
    "    x2 = np.zeros(n_iterations)\n",
    "    x3 = np.zeros(n_iterations)\n",
    "    x4 = np.zeros(n_iterations)\n",
    "    x5 = np.zeros(n_iterations)\n",
    "\n",
    "    # Define the rate lambda for the exponential distribution\n",
    "    lambda_rate = 2\n",
    "\n",
    "    # Generate the noise processes e1t, e2t, e3t, e4t, e5t\n",
    "    e1 = norm.rvs(scale=1, size=n_iterations)  # Gaussian with mean 0, std 1\n",
    "    e2 = np.random.exponential(scale=1 / lambda_rate, size=n_iterations)\n",
    "    e3 = beta.rvs(a=1, b=2, size=n_iterations)\n",
    "    e4 = beta.rvs(a=2, b=1, size=n_iterations)\n",
    "    e5 = norm.rvs(scale=1, size=n_iterations)  # Gaussian with mean 0, std 1\n",
    "\n",
    "    for t in range(3, n_iterations):\n",
    "        # Generate the x variables based on the given equations\n",
    "        x1[t] = 0.7 * x1[t - 1] + e1[t]\n",
    "        x2[t] = 0.3 * np.power(x1[t - 2], 2) + e2[t]\n",
    "        x3[t] = 0.4 * x1[t - 3] - 0.3 * x3[t - 2] + e3[t]\n",
    "        x4[t] = 0.7 * x4[t - 1] - 0.3 * x5[t - 1] * np.exp((-math.pow(x5[t - 1], 2)) / 2) + e4[t]\n",
    "        x5[t] = 0.5 * x4[t - 1] + 0.2 * x5[t - 2] + e5[t]\n",
    "\n",
    "    data = np.array([x1, x2, x3, x4, x5]).T\n",
    "    PLOT_VAR_NAMES = np.arange(5) + 1\n",
    "    PLOT_VAR_IDXS = np.arange(5)\n",
    "    df = pd.DataFrame(data, columns=PLOT_VAR_NAMES)\n",
    "    df[\"Datetime\"] = pd.date_range(start=\"1/1/2020\", periods=df.shape[0], freq=\"ms\")\n",
    "\n",
    "    dset = stf.data.CSVTimeSeries(\n",
    "        data_path=None,\n",
    "        raw_df=df,\n",
    "        val_split=0.2,\n",
    "        test_split=0.2,\n",
    "        normalize=True,\n",
    "        time_col_name=\"Datetime\",\n",
    "        time_features=[\"minute\", 'second', 'millisecond'],\n",
    "    )\n",
    "    yc_dim = data.shape[1]\n",
    "    yt_dim = data.shape[1]\n",
    "    x_dim = dset.time_cols.shape[0]\n",
    "\n",
    "    DATA_MODULE = stf.data.DataModule(\n",
    "        datasetCls=stf.data.CSVTorchDset,\n",
    "        dataset_kwargs={\n",
    "            \"csv_time_series\": dset,\n",
    "            \"context_points\": config['context_points'],\n",
    "            \"target_points\": config['target_points'],\n",
    "            \"time_resolution\": config['time_resolution'],\n",
    "        },\n",
    "        batch_size=config['batch_size'],\n",
    "        workers=config['workers'],\n",
    "        overfit=False,\n",
    "    )\n",
    "\n",
    "    return DATA_MODULE\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        gpus=default_config['gpus'],\n",
    "        callbacks=[],\n",
    "\n",
    "        accelerator=\"dp\",\n",
    "        gradient_clip_val=default_config['grad_clip_norm'],\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        overfit_batches= 0,\n",
    "        accumulate_grad_batches=default_config['accumulate'],\n",
    "        sync_batchnorm=False,\n",
    "        limit_val_batches=default_config['limit_val_batches'],\n",
    "        max_epochs=default_config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        val_check_interval = default_config['val_check_interval'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run-20240131_113012-94308ezc == s2\n",
    "\n",
    "run-20240131_120438-yj71byzb == s2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecaster\n",
      "\tL2: 1e-06\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n",
      "GlobalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "GlobalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "LocalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "LocalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0, inplace=False)\n",
      ")\n",
      "Using Embedding: spatio-temporal\n",
      "Time Emb Dim: 3\n",
      "Space Embedding: True\n",
      "Time Embedding: True\n",
      "Val Embedding: True\n",
      "Given Embedding: True\n",
      "Null Value: None\n",
      "Pad Value: None\n",
      "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1\n",
      " *** Spacetimeformer (v1.5) Summary: *** \n",
      "\t\tModel Dim: 100\n",
      "\t\tFF Dim: 400\n",
      "\t\tEnc Layers: 1\n",
      "\t\tDec Layers: 1\n",
      "\t\tEmbed Dropout: 0.2\n",
      "\t\tFF Dropout: 0.3\n",
      "\t\tAttn Out Dropout: 0\n",
      "\t\tAttn Matrix Dropout: 0\n",
      "\t\tQKV Dropout: 0\n",
      "\t\tL2 Coeff: 1e-06\n",
      "\t\tWarmup Steps: 0\n",
      "\t\tNormalization Scheme: batch\n",
      "\t\tAttention Time Windows: 1\n",
      "\t\tShifted Time Windows: False\n",
      "\t\tPosition Emb Type: abs\n",
      "\t\tRecon Loss Imp: 0\n",
      " ***                                  *** \n",
      "Forecaster\n",
      "\tL2: 1e-06\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n",
      "GlobalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "GlobalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): FullAttention(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Using Embedding: spatio-temporal\n",
      "Time Emb Dim: 6\n",
      "Space Embedding: True\n",
      "Time Embedding: True\n",
      "Val Embedding: True\n",
      "Given Embedding: True\n",
      "Null Value: None\n",
      "Pad Value: None\n",
      "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
      " *** Spacetimeformer (v1.5) Summary: *** \n",
      "\t\tModel Dim: 100\n",
      "\t\tFF Dim: 400\n",
      "\t\tEnc Layers: 1\n",
      "\t\tDec Layers: 1\n",
      "\t\tEmbed Dropout: 0.2\n",
      "\t\tFF Dropout: 0.3\n",
      "\t\tAttn Out Dropout: 0.0\n",
      "\t\tAttn Matrix Dropout: 0.0\n",
      "\t\tQKV Dropout: 0.0\n",
      "\t\tL2 Coeff: 1e-06\n",
      "\t\tWarmup Steps: 0\n",
      "\t\tNormalization Scheme: batch\n",
      "\t\tAttention Time Windows: 1\n",
      "\t\tShifted Time Windows: False\n",
      "\t\tPosition Emb Type: abs\n",
      "\t\tRecon Loss Imp: 0.0\n",
      " ***                                  *** \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Spacetimeformer_Forecaster(\n",
       "  (spacetimeformer): Spacetimeformer(\n",
       "    (enc_embedding): Embedding(\n",
       "      (data_drop): Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1\n",
       "      (time_emb): Time2Vec()\n",
       "      (local_emb): Embedding(33, 100)\n",
       "      (val_time_emb): Linear(in_features=10, out_features=100, bias=True)\n",
       "      (space_emb): Embedding(5, 100)\n",
       "      (given_emb): Embedding(2, 100)\n",
       "      (downsize_convs): ModuleList()\n",
       "    )\n",
       "    (dec_embedding): Embedding(\n",
       "      (time_emb): Time2Vec()\n",
       "      (local_emb): Embedding(33, 100)\n",
       "      (val_time_emb): Linear(in_features=10, out_features=100, bias=True)\n",
       "      (space_emb): Embedding(5, 100)\n",
       "      (given_emb): Embedding(2, 100)\n",
       "      (downsize_convs): ModuleList()\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (local_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (global_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (conv1): Conv1d(100, 400, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(400, 100, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm2): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm3): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (dropout_ff): Dropout(p=0.3, inplace=False)\n",
       "          (dropout_attn_out): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (conv_layers): ModuleList()\n",
       "      (norm_layer): Normalization(\n",
       "        (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (local_self_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (global_self_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (global_cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (local_cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (key_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (value_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (out_projection): Linear(in_features=100, out_features=100, bias=True)\n",
       "            (dropout_qkv): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (conv1): Conv1d(100, 400, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(400, 100, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm2): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm3): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm4): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (norm5): Normalization(\n",
       "            (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (dropout_ff): Dropout(p=0.3, inplace=False)\n",
       "          (dropout_attn_out): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layer): Normalization(\n",
       "        (norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (forecaster): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (reconstructor): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (classifier): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = s2_data(default_config)\n",
    "model = create_model(default_config, 3, 5, 5)\n",
    "model.set_inv_scaler(data_module.dataset_kwargs['csv_time_series'].reverse_scaling)\n",
    "model.set_scaler(data_module.dataset_kwargs['csv_time_series'].apply_scaling)\n",
    "model.set_null_value(None)\n",
    "s2_path = \"/home/dan/Documents/images/singleTokenPreds/paper_nets/S2_08917f58/S2epoch=09.ckpt\"\n",
    "s2 = model.load_from_checkpoint(s2_path)\n",
    "# results = trainer.test(model=s2, datamodule=data_module, verbose=True)\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 5])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = next(iter(data_module.test_dataloader()))\n",
    "xc, yc, xt, y_true = test_samples\n",
    "b = yc.shape[0]\n",
    "s = yc.shape[1]\n",
    "d = yc.shape[2]\n",
    "yc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_pred = model.predict(xc, yc, xt, scale_input=False, scale_output=False)\n",
    "yt_pred = yt_pred.detach().cpu().numpy()\n",
    "print(yt_pred.shape)\n",
    "yt_pred = yt_pred.reshape(-1, 5)\n",
    "yt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"//media/dan/Data/git/spacetimeformer/true.npy\", \"rb\") as f:\n",
    "    true = np.load(f)\n",
    "\n",
    "with open(\"//media/dan/Data/git/spacetimeformer/preds.npy\", \"rb\") as f:\n",
    "    preds = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcast_output close: False\n"
     ]
    }
   ],
   "source": [
    "# check if yc is the same as the test data\n",
    "\n",
    "print(f\"forcast_output close: {np.allclose(forcast_output_fromlightning, forcast_output_fromlightning1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = b # exclude the last sequence\n",
    "start_point = s + 1 # drop the first sequence \n",
    "test_data = data_module.dataset_kwargs['csv_time_series'].test_data[[1,2,3,4,5]].values[start_point:start_point+total_length, :]\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(test_data, label=\"x1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(test_data, label=\"x1\");\n",
    "plt.plot(yt_pred[:,0], label=\"x1_pred\", linestyle='dashed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(model=s2, datamodule=data_module, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "for data in iter(data_module.test_dataloader()):\n",
    "    xc, yc, xt, truth = data\n",
    "    yt_pred = model.predict(xc, yc, xt, scale_input=False, scale_output=False)\n",
    "    yt_pred = yt_pred.detach().cpu().numpy()\n",
    "    yt_pred = yt_pred.reshape(-1, 5)\n",
    "    truth = truth.detach().cpu().numpy()\n",
    "    mse.append(stf.eval_stats.mse(truth, yt_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
